{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves import cPickle as pickle\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import vgg16\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "import math\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = tf.placeholder(\"float\", [None, 224, 224, 3])\n",
    "vgg = vgg16.Vgg16()\n",
    "with tf.name_scope(\"content_vgg\"):\n",
    "    vgg.build(images)\n",
    "def vgg_result(batch):\n",
    "    n=batch.shape[0]\n",
    "    prob=np.ndarray((0, 1000), dtype=np.float32)\n",
    "    i=0   \n",
    "    with tf.Session(\n",
    "            config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.7)))) as sess:\n",
    "        while  i<n :\n",
    "            j=min(i+100,n)\n",
    "            print ('1')\n",
    "            if j<n:\n",
    "              feed_dict = {images: batch[i:j,:,:,:]}\n",
    "            else: \n",
    "              feed_dict={images:batch[i:,:,:,:]}\n",
    "           \n",
    "            prob1 = sess.run(vgg.prob, feed_dict=feed_dict)\n",
    "            prob=np.concatenate((prob,prob1),axis=0)\n",
    "            i=j\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [],
   "source": [
    "image_size = 224 # Pixel width and height.\n",
    "train_folders = [\n",
    "    os.path.join('train_data', d) for d in sorted(os.listdir('train_data')) if os.path.isdir(os.path.join('train_data',d))]\n",
    "\n",
    "'''test_folders=[\n",
    "    os.path.join('test_data', d) for d in sorted(os.listdir('test_data'))]'''\n",
    "\n",
    "def load_image(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size,3),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      img=  skimage.io.imread(image_file)\n",
    "      img=img/255.0\n",
    "      assert  (0 <= img).all() and (img <= 1.0).all()\n",
    "       \n",
    "      if len(img.shape)==2:\n",
    "        img.resize(img.shape[0],img.shape[1],1)\n",
    "        img=np.repeat(img,3,2)\n",
    "      elif img.shape[2]>3:\n",
    "        img=img[:,:,0:3]\n",
    "      short_edge = min(img.shape[:2])\n",
    "      yy = int((img.shape[0] - short_edge) / 2)\n",
    "      xx = int((img.shape[1] - short_edge) / 2)\n",
    "      crop_img = img[yy: yy + short_edge, xx: xx + short_edge,:]\n",
    "    # resize to 224, 224\n",
    "      resized_img = skimage.transform.resize(crop_img, (224, 224))\n",
    "      image_data=resized_img \n",
    "      #if image_data.shape!=(image_size,image_size,3):\n",
    "         #break\n",
    "      dataset[num_images, :, :,:] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "   # except ValueError as e:\n",
    "    #  print ('Could not load')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :,:]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "\n",
    "  return vgg_result(dataset)\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_image(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 1)\n",
    "#test_datasets = maybe_pickle(test_folders, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [],
   "source": [
    "def make_arrays(percent):\n",
    "  if percent>0.01:\n",
    "    dataset = np.ndarray((0, 1000), dtype=np.float32)\n",
    "    labels = np.ndarray((0,), dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, valid_percent,test_percent):\n",
    "  num_classes = len(pickle_files)\n",
    "  train_dataset,train_labels=make_arrays(1-valid_percent-test_percent)\n",
    "  valid_dataset,valid_labels=make_arrays(valid_percent)\n",
    "  test_dataset,test_labels=make_arrays(test_percent)\n",
    " \n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      print(label)\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "       # np.random.shuffle(letter_set)\n",
    "        print(label)\n",
    "        temp=-1\n",
    "        if valid_dataset is not None:\n",
    "          temp=int(letter_set.shape[0]*valid_percent)\n",
    "          temp1=letter_set[0:temp, :]\n",
    "          temp2=np.ones([temp,])*label\n",
    "          valid_dataset=np.concatenate((valid_dataset,temp1),axis=0)\n",
    "          valid_labels=np.concatenate((valid_labels,temp2),axis=0)\n",
    "        if test_dataset is not None:\n",
    "          temp_1=int(letter_set.shape[0]*test_percent)\n",
    "          temp1_1=letter_set[temp:temp_1+temp, :]\n",
    "          temp2_1=np.ones([temp_1,])*label\n",
    "          test_dataset=np.concatenate((test_dataset,temp1_1),axis=0)\n",
    "          test_labels=np.concatenate((test_labels,temp2_1),axis=0)\n",
    "        temp3= letter_set[temp+temp_1:, :]\n",
    "        temp4=np.ones([temp3.shape[0],])*label\n",
    "        train_dataset=np.concatenate((train_dataset,temp3),axis=0)\n",
    "        train_labels=np.concatenate((train_labels,temp4),axis=0)\n",
    "        print (train_dataset.shape)\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return test_dataset,test_labels,valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "test_dataset,test_labels,valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, 0.1,0.1)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "pickle_file = 'indoor_outdoor.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "     'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global num_labels\n",
    "image_size = 224\n",
    "num_labels = max(train_labels)+1\n",
    "\n",
    "def reformat(labels):\n",
    "\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return labels\n",
    "train_labels = reformat(train_labels)\n",
    "valid_labels = reformat(valid_labels)\n",
    "test_labels = reformat(test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "pickle_file = 'indoor_outdoor_1.pickle'\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
